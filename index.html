<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Convolutional Generative Adversarial Network</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
    <script src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="shortcut icon" href="image/image_at_epoch_0048.png">
    <link rel="stylesheet" href="css/main.css">
</head>
<body>
    <header>
        <div class="container">
          <div class="header-top">
            <div class="menu"><i class="fa-solid fa-bars"></i></div>
            <ul class="navbar">
              <li class="nav-item"><a href="#" onclick="return slowScroll('#theory')">Teoria</a></li>
              <li class="nav-item"><a href="#" onclick="return slowScroll('#research')">Badania</a></li>
              <li class="nav-item"><a href="#" onclick="return slowScroll('#uath')">Autorki</a></li>
              <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
          <div class="mobile-menu">
            <ul>
                <li class="nav-item"><a href="#" onclick="return slowScroll('#theory')">Teoria</a></li>
                <li class="nav-item"><a href="#" onclick="return slowScroll('#research')">Badania</a></li>
                <li class="nav-item"><a href="#" onclick="return slowScroll('#uath')">Autorki</a></li>
                <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
        </div>
    </header>
    <hr>
    <div class="nav" id="theory">
        <h2>Teoria</h2>
    </div>
    <div class="text">
        <div class="col-md-8">
            <p class="txt">
            Generatywna sieć przeciwstawna (z ang. Generative Adversarial Nets), czyli w skrócie GAN, która została naszym obiektem badań
            w wybranym tutorialu. GAN, to sieć, która składa się z dwóch modeli – generatora
            i dyskryminatora. Strategiia uczenia się sieci polega
            na jednoczesnym trenowaniu generatora i dyskryminatora, które skutkuje wzajemnym
            udoskonalaniem obu modeli. Schemat działania sieci został przedstawiony poniżej. Gdzie G - to generator, a D - to dyskryminator.
            </p>
            <div class="video-container mt-4">
                <video autoplay loop>
                    <source src="image/GAN.png" type="video/mp4">
                </video>
                <img src="image/GAN.png" alt="Picture for replace">
            </div>
            <p class="podpis"> Schemat budowy sieci GAN, zmodyfikowany przez A. Vezdenetską
                Źródło: https://oiot.pl/gan-czyli-jak-nauczyc-komputer-kreatywnego-myslenia/ </p>
            <br>
            <p class="txt">
                Generator doskonali się w generowaniu danych
                syntetycznych w celu osiągnięcia wyniku, jak najbardziej zbliżonego perfekcji, żeby
                uniemożliwić rozróżnienie wygenerowanego obrazu od rzeczywistych danych. Z kolei
                dyskryminator udoskonala się w poprawnym klasyfikowaniu obrazu otrzymanego
                od generatora, żeby celnie odróżnić go od danych zbioru uczącego.
            </p>
            <p class="txt">
            Podczas trenowania sieci generator, odpowiedzialny za tworzenie obrazów, uczy się jak najlepiej odwzorować odręcznie napisane cyfry. W tym samym czasie dyskryminator, odpowiedzialny za rozpoznawanie prawdziwych obrazów z zestawu próbek otrzymanych od generatora, uczy się jak rozpoznawać je z większą skutecznością, jednocześnie wymuszając większy rozwój generatora.
            </p>
            <p class="txt">
            Powoduje to otrzymywanie na wyjściu obrazu lepszej jakości. Do “nauki” danej sieci konwolucyjnej została wykorzystana baza danych MNIST (zang. Modified National Institute of Standards and Technology), która powstała jeszcze w 1998 roku i od tej pory była wielokrotnie modyfikowana. Zbiór danych zawiera około sześćdziesięciu tysięcy pojedynczych, odręcznie pisanych obrazów arabskich cyfr, zaklasyfikowanych jako treningowe oraz dziesięciu tysięcy obrazów zaklasyfikowanych jako testowe. Każdy z tych obrazów został wyśrodkowany w celu późniejszego rozmieszczenia powstałych cyfr w centrum, a także dostosowany do wymiaru 28x28pikseli, co czyni daną bazę danych idealnie dopasowaną do potrzeb konkretnego GANa.
            </p>
            <p class="txt">
            Na podstawie MNIST generator tworzy figury przypominające obrazy odręcznie pisanych cyfr, podczas gdy dyskryminator staje się lepszy w ich wykrywaniu i odróżnianiu od obrazów zawartych w dostępnej bazie danych. Kody generatora oraz dyskryminatora bazują na kodzie, który pochodzi z otwartej biblioteki Keras, umożliwiającej tworzenie interfejsów dla sieci neuronowych w języku programowania Python.
            </p>
            <p class="txt">
            Przy użyciu generatywnej sieci przeciwstawnej opisanej w samouczku udostępnionym na platformie TensorFlow, z powodzeniem udało się wytworzyć obrazy z cyframi. Następnie zostały wprowadzone zmiany w wybranych fragmentach kodu w celu pokazania ich wpływu na wyniki końcowe. Badania nad konkretnymi wartościami i ich modyfikacjami zostały opisane i przedstawione wraz z wynikami w kolejnych punktach.
            </p>
            <br>
        </div>
    </div>
    <hr>
    <div class="nav" id="research">
        <h2>Badania</h2>
    </div>
    <div class="container">
        <div class="row">
            <div class="col-md-4">
                <div class="card">
                    <div class="card-body">
                        <h4>Badanie zmiennej funkcji aktywacji w warstwie wyjściowej</h4>
                        <p>
                            Dany eksperyment miał na celu zbadanie wpływu zmiany funkcji aktywacji w warstwie wyjściowej na otrzymane dane końcowe...
                        </p>
                        <img src="image/gif/activation/linear.gif" class="card-img-top" alt="picture linear function">
                        <a href="pages/activation.html" class="btn btn-warning">Więcej</a>
                   </div>
                </div>
                <div class="card mt-4">
                    <div class="card-body">
                        <h4>Badanie wpływu zmiany ilości kroków filtra na wynik końcowy</h4>
                        <p>
                            Przeprowadzony eksperyment, polegał na sprawdzeniu jaki wpływ ma zmiana wartości danych wejściowych kroku filtrów na dane wyjściowe...
                        </p>
                        <img src="image/gif/stride/1.gif" class="card-img-top">
                        <a href="pages/stride.html" class = "btn btn-warning">Więcej</a>
                    </div>
                </div>
            </div>
            <div class="col-md-4">
                <div class="card">
                    <div class="card-body">
                        <h4>Badanie wpływu ilości filtrów na wynik końcowy</h4>
                        <p>
                            W tym badaniu został przeprowadzony eksperyment związany ze zmianą danych, określających rozmiar macierzy...
                        </p>
                        <img src="image/gif/kernelfilter/filter 64, 128/5.gif" class="card-img-top">
                        <a href="pages/filter.html" class = "btn btn-warning">Więcej</a>
                   </div>
                </div>
                <div class="card mt-4">
                    <div class="card-body">
                        <h4>Badanie zależności między rozmiarem wsadu, a rozmiarem bufora</h4>
                        <p>
                            W celu weryfikacji tego czy istnieje współuzależnienie między rozmiarem bufora, a rozmiarem wsadu, wartości...
                        </p>
                        <img src="image/gif/gif256/bufer 60000.gif" class="card-img-top">
                        <a href="pages/batch_bufer.html" class = "btn btn-warning">Więcej</a>
                    </div>
                </div>
            </div>
            <div class="col-md-4">
                <div class="card">
                    <div class="card-body">
                        <h4>Badanie wpływu zmiany rozmiaru filtrów na wynik końcowy</h4>
                        <p>
                            Badanie to miało na celu sprawdzenie jaki wpływ na wynik końcowy trenowania sieci będzie miała zmiana wyjściowej...
                        </p>
                        <img src="image/gif/kernelfilter/filter 8, 16/(5, 5).gif" class="card-img-top">
                        <a href="pages/kernel_size.html" class = "btn btn-warning">Więcej</a>
                   </div>
                </div>
                <div class="card mt-4">
                    <div class="card-body">
                        <h4>Badanie zależności pomiędzy zmienną filter, a zmienną kernel size</h4>
                        <p>
                            Zestawiono ze sobą wyniki zmiany wartości danych wyjściowych ilości filtrów i jednoczesne zmiany wartości danych wyjściowych...
                        </p>
                        <img src="image/gif/kernelfilter/filter 32, 64/(3,3).gif" class="card-img-top">
                        <a href="pages/kernel_filter.html" class = "btn btn-warning">Więcej</a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <hr>
    <div class="nav" id="uath">
        <h2>Autorki</h2>
    </div>
    <div class="container">
        <div class="row">
            <div class="col-md-6">
                <div class="authore">
                    <a href="https://www.linkedin.com/in/anna-vezdenetska-a78512207/" target="_blank">
                        <img src="image/1714665977716.jpg" alt="Koala">
                        <h2>Anna vezdenetska</h2>
                    </a>
                    <p><i class="fa-solid fa-phone"></i>  +380668667061</p>
                    <p><i class="fa-solid fa-envelope"></i>  anna.vezdenetska@gmail.com</p>
                </div>
            </div>
            <div class="col-md-6">
                <div class="authore">
                    <a href="https://www.facebook.com/ola.kot.1441" target="_blank">
                        <img src="image/100600059_2782086938587553_6254879392787333120_n.jpg" alt="Kot">
                        <h2>Aleksandra Kot</h2>
                    </a>
                    <p><i class="fa-solid fa-phone"></i>  +48505646888</p>
                    <p><i class="fa-solid fa-envelope"></i>  isness.hoover@gmail.com</p>
                </div>
            </div>
        </div>
    </div>


    <hr>
    <div class="nav" id="code">
        <h2>Kod źródłowy</h2>
    </div>
    <div class="wiecej">
        <p>
            <blockquote cite="https://www.tensorflow.org/tutorials/generative/dcgan">
                <pre>
                import tensorflow as tf
            
                import glob
                import imageio
                import matplotlib.pyplot as plt
                import numpy as np
                import os
                import PIL
                from tensorflow.keras import layers
                import time
            
                from IPython import display
            
                def make_generator_model():
                    model = tf.keras.Sequential()
                    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
                    model.add(layers.BatchNormalization())
                    model.add(layers.LeakyReLU())
                    model.add(layers.Reshape((7, 7, 256)))
                    assert model.output_shape == (None, 7, 7, 256)
                    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
                    assert model.output_shape == (None, 7, 7, 128)
                    model.add(layers.BatchNormalization())
                    model.add(layers.LeakyReLU())
                    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
                    assert model.output_shape == (None, 14, 14, 64)
                    model.add(layers.BatchNormalization())
                    model.add(layers.LeakyReLU())
                    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2),
                    padding='same', use_bias=False, activation='tanh'))
                    assert model.output_shape == (None, 28, 28, 1)
                    return model
            
                def make_discriminator_model():
                    model = tf.keras.Sequential()
                    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                    input_shape=[28, 28, 1]))
                    model.add(layers.LeakyReLU())
                    model.add(layers.Dropout(0.3))
                    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
                    model.add(layers.LeakyReLU())
                    model.add(layers.Dropout(0.3))
                    model.add(layers.Flatten())
                    model.add(layers.Dense(1))
            
                    return model
            
                def discriminator_loss(real_output, fake_output):
                    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
                    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
                    total_loss = real_loss + fake_loss
                    return total_loss
            
                def generator_loss(fake_output):
                    return cross_entropy(tf.ones_like(fake_output), fake_output)
            
                @tf.function
                def train_step(images):
                    noise = tf.random.normal([BATCH_SIZE, noise_dim])
            
                    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                    generated_images = generator(noise, training=True)
            
                    real_output = discriminator(images, training=True)
                    fake_output = discriminator(generated_images, training=True)
            
                    gen_loss = generator_loss(fake_output)
                    disc_loss = discriminator_loss(real_output, fake_output)
            
                    gradients_of_generator = gen_tape.gradient(
                    gen_loss, generator.trainable_variables)
                    gradients_of_discriminator = disc_tape.gradient(
                    disc_loss, discriminator.trainable_variables)
            
                    generator_optimizer.apply_gradients(
                    zip(gradients_of_generator, generator.trainable_variables))
                    discriminator_optimizer.apply_gradients(
                    zip(gradients_of_discriminator, discriminator.trainable_variables))
            
                def generate_and_save_images(model, epoch, test_input):
            
                predictions = model(test_input, training=False)
                fig = plt.figure(figsize=(4, 4))
            
                for i in range (predictions.shape[0]):
                    plt.subplot(4, 4, i+1)
                    plt.imshow (predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
                    plt.axis('off')
            
                plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
            
                def train(dataset, epochs):
                for epoch in range(epochs):
                    start = time.time()
            
                    for image_batch in dataset:
                    train_step(image_batch)
            
                    display.clear_output(wait=True)
                    generate_and_save_images(generator,
                                            epoch + 1,
                                            seed)
            
                    if (epoch + 1) % 3 == 0:
                    checkpoint.save(file_prefix=checkpoint_prefix)
            
                    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))
            
                display.clear_output(wait=True)
                generate_and_save_images(generator,
                                            epochs,
                                            seed)
            
                (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()
            
                train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
                train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]
                BUFFER_SIZE = 60000
                BATCH_SIZE = 256
            
                train_dataset = tf.data.Dataset.from_tensor_slices(
                    train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
            
                generator = make_generator_model()
            
                noise = tf.random.normal([1, 100])
                generated_image = generator(noise, training=False)
            
                discriminator = make_discriminator_model()
                decision = discriminator(generated_image)
                print(decision)
            
                cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
            
                generator_optimizer = tf.keras.optimizers.Adam(1e-4)
                discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)
            
                checkpoint_dir = './training_checkpoints'
                checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
                checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                                discriminator_optimizer=discriminator_optimizer,
                                                generator=generator,
                                                discriminator=discriminator)
            
                EPOCHS = 50
                noise_dim = 100
                num_examples_to_generate = 16
            
                seed = tf.random.normal([num_examples_to_generate, noise_dim])
            
                train(train_dataset, EPOCHS)
                </pre>
            <br><br>
            </blockquote>
        </p>
    </div>
    <footer>
        <ul>
            <li class="nav-item"><a href="#" onclick="return slowScroll('#theory')">Teoria</a></li>
            <li class="nav-item"><a href="#" onclick="return slowScroll('#research')">Badania</a></li>
            <li class="nav-item"><a href="#" onclick="return slowScroll('#uath')">Autorki</a></li>
            <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
        </ul>
    </footer>  
      <script src="https://kit.fontawesome.com/9f85493fa2.js" crossorigin="anonymous"></script>
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
      <script src="js/main.js"></script>
      <script src="js/menu.js"></script>
</body>
</html>