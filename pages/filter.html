<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Convolutional Generative Adversarial Network</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
    <script src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="shortcut icon" href="../image/image_at_epoch_0048.png">
    <link rel="stylesheet" href="../css/main.css">
    <link rel="stylesheet" href="../css/activation.css">
</head>
<body>
    <header>
        <div class="container">
          <div class="header-top">
            <div class="menu"><i class="fa-solid fa-bars"></i></div>
            <ul class="navbar">
              <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
              <li class="nav-item"><a href="../index.html#research">Badania</a></li>
              <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
              <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
          <div class="mobile-menu">
            <ul>
                <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
                <li class="nav-item"><a href="../index.html#research">Badania</a></li>
                <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
                <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
        </div>
    </header>
    <hr>
    <div class="nav">
        <div class="col-md-6">
            <h2>Badanie wpływu ilości filtrów na wynik końcowy</h2>
        </div>
    </div>
    <div class="text">
        <div class="col-md-8">
            <p class="txt">
                Filtr (z ang. filter) to macierz, czyli zestaw danych, który nałożonyna dane wejściowe zmienia je, zapamiętując zbiór cech z konkretnego zestawienia np.ostre krawędzie. Po nałożeniu wielu filtrów powstaje obraz końcowy, będący wynikiemnawarstwionych zbiorów cech połączonych ze sobą.
            </p>
            <p class="txt">
                Badanie to miało na celusprawdzenie jaki wpływ na wynik końcowy trenowania sieci będzie miała zmianawyjściowej ilości filtrów nałożonych na obraz. Dane zostały zmienione w obu warstwachkonwolucyjnych (layers.Conv2D) przy zachowaniu zasady dwukrotności wartościw kolejnych warstwach. Przed rozpoczęciem badania wyszliśmy z założenia że im więcej będzie filtrów, tym większa będzie ostrość otrzymanych figur.Wyniki powstałe z tego badania zostały przedstawione w tabeli poniżej.
            </p>
            <table class="tab">
                <thead>
                   <tr>
                      <th>Ilość filtrów: 8 i 16</th>
                      <th>Ilość filtrów: 16 i 32</th>
                      <th>Ilość filtrów: 32 i 64</th>
                      <th>Ilość filtrów: 64 i 128</th>
                      <th>Ilość filtrów: 128 i 256</th>
                   </tr>
                </thead>
                <tbody>
                   <tr>
                      <td><img src="../image/gif/kernelfilter/filter 8, 16/(5, 5).gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 16, 32/(5, 5).gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 32, 64/(5, 5).gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 64, 128/5.gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 128, 256/5.gif"/></td>
                   </tr>
                </tbody>
             </table>
            <p class="podpis"> Opracowanie własne A. Kot</p>
            <div class="nav">
                <h2>Wnioski</h2>
            </div>
            <p class="txt">
                Wartościami podstawowymi dla obu warstw konwolucyjnych były kolejno 64 filtryi 128. Najniższe wyniki porównane ze sobą, czyli 8 i 16 filtrów oraz 16 i 32 filtry, jasnowskazują, że im niższa jest ilość nałożonych filtrów, tym obraz końcowy jest mniejczytelny. Wygenerowane figury utraciły swój kształt, a miejscami wręcz nieprzypominają cyfr, powodując tym samym duże trudności w ich odczytaniu.
            </p>
            <p class="txt">
                Wynik przedstawia się inaczej w przypadku zwiększenia wartości ilości nakładanychfiltrów do 128 i 256. W tym przypadku obraz końcowy zdaje się być wyraźniejszy,a figury łatwiejsze do rozpoznania i odczytu.
            </p>            
            <p class="txt">
                Przy ustawieniu takich wartości dla zmiennych pojawiał się jednak problemprzetrenowania sieci, który z większą dokładnością można zauważyć przy kolejnychwartościach 256 i 512. Wynik przedstawiony w tabeli powyżej wskazuje na to, że figurystają się bardziej “rozmyte”, czyli nie mają one ostrych krawędzi, co także prowadzido tego, że dane figury stają się trudniejsze do odczytania. Na tym etapie sieć zaczęłazapamiętywać schematy konkretnych cech powielając je przy kolejnych epokach, przezco obraz końcowy przedstawia się gorzej niż w poprzednich dwóch wariantachzmienionych danych ilości filtrów.
            </p>
        </div>
    </div>
    <hr>
    <div class="nav" id="code">
        <div class="col-md-6">
            <h2>Kod źródłowy, z zaznaczonymi miejscami wprowadzenia zmian</h2>
        </div>
    </div>
    <div class="wiecej">
        <p>
            <blockquote cite="https://www.tensorflow.org/tutorials/generative/dcgan">
                <pre>
                    import tensorflow as tf
                    import glob
                    import imageio
                    import matplotlib.pyplot as plt
                    import numpy as np
                    import os
                    import PIL
                    from tensorflow.keras import layers
                    import time

                    from IPython import display

                    def make_generator_model():
                        model = tf.keras.Sequential()
                        model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Reshape((7, 7, 256)))
                        assert model.output_shape == (None, 7, 7, 256)
                        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
                        assert model.output_shape == (None, 7, 7, 128)
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
                        assert model.output_shape == (None, 14, 14, 64)
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2),
                        padding='same', use_bias=False, activation='tanh'))

                        assert model.output_shape == (None, 28, 28, 1)
                        return model

                        
                    def make_discriminator_model():
                        model = tf.keras.Sequential()
                        model.add(layers.Conv2D(<mark>64,</mark> (5, 5), strides=(2, 2), padding='same',
                        input_shape=[28, 28, 1]))
                        model.add(layers.LeakyReLU())
                        model.add(layers.Dropout(0.3))
                        model.add(layers.Conv2D(<mark>128,</mark> (5, 5), strides=(2, 2), padding='same'))
                        model.add(layers.LeakyReLU())
                        model.add(layers.Dropout(0.3))
                        model.add(layers.Flatten())
                        model.add(layers.Dense(1))

                        return model

                    def discriminator_loss(real_output, fake_output):
                        real_loss = cross_entropy(tf.ones_like(real_output), real_output)
                        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
                        total_loss = real_loss + fake_loss
                        return total_loss

                    def generator_loss(fake_output):
                    return cross_entropy(tf.ones_like(fake_output), fake_output)

                    @tf.function
                    def train_step(images):
                        noise = tf.random.normal([BATCH_SIZE, noise_dim])

                        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                        generated_images = generator(noise, training=True)

                        real_output = discriminator(images, training=True)
                        fake_output = discriminator(generated_images, training=True)

                        gen_loss = generator_loss(fake_output)
                        disc_loss = discriminator_loss(real_output, fake_output)

                        gradients_of_generator = gen_tape.gradient(
                        gen_loss, generator.trainable_variables)
                        gradients_of_discriminator = disc_tape.gradient(
                        disc_loss, discriminator.trainable_variables)

                        generator_optimizer.apply_gradients(
                        zip(gradients_of_generator, generator.trainable_variables))
                        discriminator_optimizer.apply_gradients(
                        zip(gradients_of_discriminator, discriminator.trainable_variables))

                    def generate_and_save_images(model, epoch, test_input):

                    predictions = model(test_input, training=False)
                    fig = plt.figure(figsize=(4, 4))

                    for i in range (predictions.shape[0]):
                        plt.subplot(4, 4, i+1)
                        plt.imshow (predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
                        plt.axis('off')

                    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))

                    def train(dataset, epochs):
                    for epoch in range(epochs):
                        start = time.time()

                        for image_batch in dataset:
                        train_step(image_batch)

                        display.clear_output(wait=True)
                        generate_and_save_images(generator,
                                                epoch + 1,
                                                seed)

                        if (epoch + 1) % 3 == 0:
                        checkpoint.save(file_prefix=checkpoint_prefix)

                        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

                    display.clear_output(wait=True)
                    generate_and_save_images(generator,
                                                epochs,
                                                seed)

                    (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()

                    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
                    train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]

                    BUFFER_SIZE = 60000
                    BATCH_SIZE = 256


                    train_dataset = tf.data.Dataset.from_tensor_slices(
                        train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

                    generator = make_generator_model()

                    noise = tf.random.normal([1, 100])
                    generated_image = generator(noise, training=False)

                    discriminator = make_discriminator_model()
                    decision = discriminator(generated_image)
                    print(decision)

                    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

                    generator_optimizer = tf.keras.optimizers.Adam(1e-4)
                    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

                    checkpoint_dir = './training_checkpoints'
                    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
                    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                                    discriminator_optimizer=discriminator_optimizer,
                                                    generator=generator,
                                                    discriminator=discriminator)

                    EPOCHS = 50
                    noise_dim = 100
                    num_examples_to_generate = 16

                    seed = tf.random.normal([num_examples_to_generate, noise_dim])

                    train(train_dataset, EPOCHS)
                    </pre>
            <br><br>
            </blockquote>
        </p>
    </div>
    
    <footer>
        <ul>
            <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
            <li class="nav-item"><a href="../index.html#research">Badania</a></li>
            <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
            <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
        </ul>
    </footer>  
      <script src="https://kit.fontawesome.com/9f85493fa2.js" crossorigin="anonymous"></script>
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
      <script src="../js/main.js"></script>
      <script src="../js/menu.js"></script>
</body>
</html>