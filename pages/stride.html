<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Convolutional Generative Adversarial Network</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
    <script src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="shortcut icon" href="../image/image_at_epoch_0048.png">
    <link rel="stylesheet" href="../css/main.css">
    <link rel="stylesheet" href="../css/activation.css">
</head>
<body>
    <header>
        <div class="container">
          <div class="header-top">
            <div class="menu"><i class="fa-solid fa-bars"></i></div>
            <ul class="navbar">
              <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
              <li class="nav-item"><a href="../index.html#research">Badania</a></li>
              <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
              <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
          <div class="mobile-menu">
            <ul>
                <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
                <li class="nav-item"><a href="../index.html#research">Badania</a></li>
                <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
                <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
        </div>
    </header>
    <hr>
    <div class="nav">
        <div class="col-md-6">
            <h2>Badanie zmiany ilości kroków filtra na wynik końcowy</h2>
        </div>
    </div>
    <div class="text">
        <div class="col-md-8">
            <p class="txt">
                Krok filtrów (z ang. stride) to ilość pikseli o jaką ma się przesuwać filtr,względem danych wejściowych. W kolejnym badaniu został przeprowadzonyeksperyment, polegający na sprawdzeniu jaki wpływ ma zmiana wartości danychwejściowych kroku filtrów na dane wyjściowe trenowanej sieci. Dane zostałyzmodyfikowane pod kątem zmniejszenia jak i zwiększenia kroku, czyli przesunięcia sięmacierzy.            
            </p>
            <table class="tab">
                <thead>
                   <tr>
                      <th>Krok splotu: (1, 1)</th>
                      <th>Krok splotu: (2, 2)</th>
                      <th>Krok splotu: (3, 3)</th>
                      <th>Krok splotu: (4, 4)</th>
                      <th>Krok splotu: (5, 5)</th>
                   </tr>
                </thead>
                <tbody>
                   <tr>
                      <td><img src="../image/gif/stride/1.gif"/></td>
                      <td><img src="../image/gif/stride/2.gif"/></td>
                      <td><img src="../image/gif/stride/3.gif"/></td>
                      <td><img src="../image/gif/stride/4.gif"/></td>
                      <td><img src="../image/gif/stride/5.gif"/></td>
                   </tr>
                </tbody>
             </table>             
            <p class="podpis"> Opracowanie własne A. Kot</p>
            <div class="nav">
                <h2>Wnioski</h2>
            </div>
            <p class="txt">
                Podstawową wartością dla zmiennej kroku filtrów jest przesunięcie o dwa pikselewzględem szerokości jak i długości dla danych wejściowych. Przy zmianie wartościkroku na jeden piksel zauważyć można na podstawie powyższej tabeli, że od stronywizualnej wynik końcowy nie różni się szczególnie względem wyniku końcowego dlawartości podstawowych. W tym przypadku różnica polegała na czasie potrzebnymdo wytrenowania jednej epoki. W przypadku wartości podstawowych wytrenowaniejednej epoki zajmowało średnio dwadzieścia sekund, natomiast po zmniejszeniu wartościzmiennej czas średni wynosił czterdzieści pięć sekund, co stanowi wydłużenie czasutrenowania o 125%.            
            </p>
            <p class="txt">
                Przy zmianie wartości kroku na trzy piksele względem szerokości i długości widocznajest zmiana w wyniku końcowym. Figury stały się mniej wyraźne, a ich kształt uległpogorszeniu. Podobna sytuacja ma miejsce przy zmianie wartości kroku kolejno na czterypiksele względem szerokości i długości oraz pięć pikseli względem szerokości i długości.Figury stały się rozmazane i “chaotyczne” względem wyniku dla wartościpodstawowych, co przekłada się bezpośrednio na trudność w ich odczytaniu.            
            </p>
        </div>
    </div>
    <hr>
    <div class="nav" id="code">
        <div class="col-md-6">
            <h2>Kod źródłowy, z zaznaczonymi miejscami wprowadzenia zmian</h2>
        </div>
    </div>
    <div class="wiecej">
        <p>
            <blockquote cite="https://www.tensorflow.org/tutorials/generative/dcgan">
                <pre>
                    import tensorflow as tf

                    import glob
                    import imageio
                    import matplotlib.pyplot as plt
                    import numpy as np
                    import os
                    import PIL
                    from tensorflow.keras import layers
                    import time

                    from IPython import display

                    def make_generator_model():
                        model = tf.keras.Sequential()
                        model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Reshape((7, 7, 256)))
                        assert model.output_shape == (None, 7, 7, 256)
                        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
                        assert model.output_shape == (None, 7, 7, 128)
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
                        assert model.output_shape == (None, 14, 14, 64)
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2),
                        padding='same', use_bias=False, activation='tanh'))

                        assert model.output_shape == (None, 28, 28, 1)
                        return model


                        
                    def make_discriminator_model():
                        model = tf.keras.Sequential()
                        model.add(layers.Conv2D(64, (5, 5), <mark>strides=(2, 2)</mark>, padding='same',
                        input_shape=[28, 28, 1]))
                        model.add(layers.LeakyReLU())
                        model.add(layers.Dropout(0.3))
                        model.add(layers.Conv2D(128, (5, 5), <mark>strides=(2, 2)</mark>, padding='same'))
                        model.add(layers.LeakyReLU())
                        model.add(layers.Dropout(0.3))
                        model.add(layers.Flatten())
                        model.add(layers.Dense(1))

                        return model

                    def discriminator_loss(real_output, fake_output):
                        real_loss = cross_entropy(tf.ones_like(real_output), real_output)
                        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
                        total_loss = real_loss + fake_loss
                        return total_loss

                    def generator_loss(fake_output):
                    return cross_entropy(tf.ones_like(fake_output), fake_output)

                    @tf.function
                    def train_step(images):
                        noise = tf.random.normal([BATCH_SIZE, noise_dim])

                        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                        generated_images = generator(noise, training=True)

                        real_output = discriminator(images, training=True)
                        fake_output = discriminator(generated_images, training=True)

                        gen_loss = generator_loss(fake_output)
                        disc_loss = discriminator_loss(real_output, fake_output)

                        gradients_of_generator = gen_tape.gradient(
                        gen_loss, generator.trainable_variables)
                        gradients_of_discriminator = disc_tape.gradient(
                        disc_loss, discriminator.trainable_variables)

                        generator_optimizer.apply_gradients(
                        zip(gradients_of_generator, generator.trainable_variables))
                        discriminator_optimizer.apply_gradients(
                        zip(gradients_of_discriminator, discriminator.trainable_variables))

                    def generate_and_save_images(model, epoch, test_input):

                    predictions = model(test_input, training=False)
                    fig = plt.figure(figsize=(4, 4))

                    for i in range (predictions.shape[0]):
                        plt.subplot(4, 4, i+1)
                        plt.imshow (predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
                        plt.axis('off')

                    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))

                    def train(dataset, epochs):
                    for epoch in range(epochs):
                        start = time.time()

                        for image_batch in dataset:
                        train_step(image_batch)

                        display.clear_output(wait=True)
                        generate_and_save_images(generator,
                                                epoch + 1,
                                                seed)

                        if (epoch + 1) % 3 == 0:
                        checkpoint.save(file_prefix=checkpoint_prefix)

                        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

                    display.clear_output(wait=True)
                    generate_and_save_images(generator,
                                                epochs,
                                                seed)

                    (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()

                    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
                    train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]

                    BUFFER_SIZE = 60000
                    BATCH_SIZE = 256


                    train_dataset = tf.data.Dataset.from_tensor_slices(
                        train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

                    generator = make_generator_model()

                    noise = tf.random.normal([1, 100])
                    generated_image = generator(noise, training=False)

                    discriminator = make_discriminator_model()
                    decision = discriminator(generated_image)
                    print(decision)

                    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

                    generator_optimizer = tf.keras.optimizers.Adam(1e-4)
                    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

                    checkpoint_dir = './training_checkpoints'
                    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
                    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                                    discriminator_optimizer=discriminator_optimizer,
                                                    generator=generator,
                                                    discriminator=discriminator)

                    EPOCHS = 50
                    noise_dim = 100
                    num_examples_to_generate = 16

                    seed = tf.random.normal([num_examples_to_generate, noise_dim])

                    train(train_dataset, EPOCHS)
                    </pre>
            <br><br>
            </blockquote>
        </p>
    </div>
    
    <footer>
        <ul>
            <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
            <li class="nav-item"><a href="../index.html#research">Badania</a></li>
            <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
            <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
        </ul>
    </footer>  
      <script src="https://kit.fontawesome.com/9f85493fa2.js" crossorigin="anonymous"></script>
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
      <script src="../js/main.js"></script>
      <script src="../js/menu.js"></script>
</body>
</html>