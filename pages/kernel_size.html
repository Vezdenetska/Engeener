<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Convolutional Generative Adversarial Network</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
    <script src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="shortcut icon" href="../image/image_at_epoch_0048.png">
    <link rel="stylesheet" href="../css/main.css">
    <link rel="stylesheet" href="../css/activation.css">
</head>
<body>
    <header>
        <div class="container">
          <div class="header-top">
            <div class="menu"><i class="fa-solid fa-bars"></i></div>
            <ul class="navbar">
              <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
              <li class="nav-item"><a href="../index.html#research">Badania</a></li>
              <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
              <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
          <div class="mobile-menu">
            <ul>
                <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
                <li class="nav-item"><a href="../index.html#research">Badania</a></li>
                <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
                <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
            </ul>
          </div>
        </div>
    </header>
    <hr>
    <div class="nav">
        <div class="col-md-6">
            <h2>Badanie wpływu zmiany rozmiaru filtrów na wynik końcowy</h2>
        </div>
    </div>
    <div class="text">
        <div class="col-md-8">
            <p class="txt">
                Rozmiar filtrów (z ang. kernel size) to wymiary określające długośći szerokość filtra. W tym badaniu został przeprowadzony eksperyment związanyze zmianą danych, określających rozmiar macierzy. Miał na celu sprawdzenie jak zmianata wpłynie na wynik końcowy trenowanej sieci przy założeniu zmniejszenia danychi zwiększenia względem wartości podstawowej, która wynosiła pięć wierszy na pięćkolumn.
            </p>
            <table class="tab">
                <thead>
                   <tr>
                      <th>Rozmiar filtra: (1, 1)</th>
                      <th>Rozmiar filtra: (3, 3)</th>
                      <th>Rozmiar filtra: (5, 5)</th>
                      <th>Rozmiar filtra: (7, 7)</th>
                      <th>Rozmiar filtra: (9, 9)</th>
                   </tr>
                </thead>
                <tbody>
                   <tr>
                      <td><img src="../image/gif/kernelfilter/filter 64, 128/1.gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 64, 128/3.gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 64, 128/5.gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 64, 128/7.gif"/></td>
                      <td><img src="../image/gif/kernelfilter/filter 64, 128/9.gif"/></td>
                   </tr>
                </tbody>
             </table>
            <p class="podpis"> Opracowanie własne A. Kot</p>
            <div class="nav">
                <h2>Wnioski</h2>
            </div>
            <p class="txt">
                Przy zmniejszeniu wartości zmiennej rozmiaru filtra do jednej kolumny i jednegowiersza, zauważyć można dużą deformację obrazu końcowego, względem danychpodstawowych.
            </p>
            <p class="txt">
                Kolejnymi danymi użytymi w badaniu były trzy kolumny i trzy wiersze. W tymprzypadku wynik końcowy prezentuje się o wiele lepiej względem poprzedniego. Tło jestwyraźnie oddzielone od figur, jednak w dalszym ciągu obraz prezentuje się gorzejwzględem wyniku dla wartości podstawowych. Jak można zauważyć na podstawie tabelifigury są krzywe, pogrubione i miejscami poszarpane na oddzielne kawałki.            
            </p>            
            <p class="txt">
                Sytuacja wygląda inaczej dla zwiększonych wartości rozmiaru filtra, wynoszącychsiedem wierszy i siedem kolumn oraz dziewięć wierszy i dziewięć kolumn. Figurysą wyraźne i w większości przypadków odczytanie ich nie stanowi wyzwania. Wizualniebadane wyniki wydają się nie różnić między sobą i danymi podstawowymi. Różnicajednak występuje w samym procesie trenowania. Średni czas trenowania epoki dla pięciukolumn i pięciu wierszy, czyli danych podstawowych wynosi około piętnastu sekund,natomiast kolejno dla wartości zmienionych, czyli siedmiu kolumn i siedmiu wierszywynosi średnio około osiemnastu sekund, co stanowi zwiększenie czasu trenowania epokio 20%, a dla wartości dziewięciu kolumn i dziewięciu wierszy średni czas wynosi jużdwadzieścia sekund, co stanowi około 33% wzrost czasu trenowania. Biorąc pod uwagęwrażenia wizualne i różnicę w czasie trenowania sieci najbardziej optymalnymiwartościami dla tej konkretnej sieci są te podstawowe, które zachowują balans pomiędzyjakością obrazu końcowego, a samym procesem trenowania.            
            </p>
        </div>
    </div>
    <hr>
    <div class="nav" id="code">
        <div class="col-md-6">
            <h2>Kod źródłowy, z zaznaczonymi miejscami wprowadzenia zmian</h2>
        </div>
    </div>
    <div class="wiecej">
        <p>
            <blockquote cite="https://www.tensorflow.org/tutorials/generative/dcgan">
                <pre>
                    import tensorflow as tf

                    import glob
                    import imageio
                    import matplotlib.pyplot as plt
                    import numpy as np
                    import os
                    import PIL
                    from tensorflow.keras import layers
                    import time
                    
                    from IPython import display
                    
                    def make_generator_model():
                        model = tf.keras.Sequential()
                        model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Reshape((7, 7, 256)))
                        assert model.output_shape == (None, 7, 7, 256)
                        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
                        assert model.output_shape == (None, 7, 7, 128)
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
                        assert model.output_shape == (None, 14, 14, 64)
                        model.add(layers.BatchNormalization())
                        model.add(layers.LeakyReLU())
                        model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2),
                        padding='same', use_bias=False, activation='tanh'))
                    
                        assert model.output_shape == (None, 28, 28, 1)
                        return model
                    
                    
                    def make_discriminator_model():
                        model = tf.keras.Sequential()
                        model.add(layers.Conv2D(64, <mark>(5, 5)</mark>, strides=(2, 2), padding='same',
                        input_shape=[28, 28, 1]))
                        model.add(layers.LeakyReLU())
                        model.add(layers.Dropout(0.3))
                        model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
                        model.add(layers.LeakyReLU())
                        model.add(layers.Dropout(0.3))
                        model.add(layers.Flatten())
                        model.add(layers.Dense(1))
                    
                        return model
                    
                    def discriminator_loss(real_output, fake_output):
                        real_loss = cross_entropy(tf.ones_like(real_output), real_output)
                        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
                        total_loss = real_loss + fake_loss
                        return total_loss
                    
                    def generator_loss(fake_output):
                       return cross_entropy(tf.ones_like(fake_output), fake_output)
                    
                    @tf.function
                    def train_step(images):
                        noise = tf.random.normal([BATCH_SIZE, noise_dim])
                    
                        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                          generated_images = generator(noise, training=True)
                    
                          real_output = discriminator(images, training=True)
                          fake_output = discriminator(generated_images, training=True)
                    
                          gen_loss = generator_loss(fake_output)
                          disc_loss = discriminator_loss(real_output, fake_output)
                    
                        gradients_of_generator = gen_tape.gradient(
                        gen_loss, generator.trainable_variables)
                        gradients_of_discriminator = disc_tape.gradient(
                        disc_loss, discriminator.trainable_variables)
                    
                        generator_optimizer.apply_gradients(
                        zip(gradients_of_generator, generator.trainable_variables))
                        discriminator_optimizer.apply_gradients(
                        zip(gradients_of_discriminator, discriminator.trainable_variables))
                    
                    def generate_and_save_images(model, epoch, test_input):
                    
                      predictions = model(test_input, training=False)
                      fig = plt.figure(figsize=(4, 4))
                    
                      for i in range (predictions.shape[0]):
                          plt.subplot(4, 4, i+1)
                          plt.imshow (predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
                          plt.axis('off')
                    
                      plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
                    
                    def train(dataset, epochs):
                      for epoch in range(epochs):
                        start = time.time()
                    
                        for image_batch in dataset:
                          train_step(image_batch)
                    
                        display.clear_output(wait=True)
                        generate_and_save_images(generator,
                                                  epoch + 1,
                                                  seed)
                    
                        if (epoch + 1) % 3 == 0:
                          checkpoint.save(file_prefix=checkpoint_prefix)
                    
                        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))
                    
                      display.clear_output(wait=True)
                      generate_and_save_images(generator,
                                                epochs,
                                                seed)
                    
                    (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()
                    
                    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
                    train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]
                    
                    BUFFER_SIZE = 60000
                    BATCH_SIZE = 256
                    
                    
                    train_dataset = tf.data.Dataset.from_tensor_slices(
                        train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
                    
                    generator = make_generator_model()
                    
                    noise = tf.random.normal([1, 100])
                    generated_image = generator(noise, training=False)
                    
                    discriminator = make_discriminator_model()
                    decision = discriminator(generated_image)
                    print(decision)
                    
                    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
                    
                    generator_optimizer = tf.keras.optimizers.Adam(1e-4)
                    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)
                    
                    checkpoint_dir = './training_checkpoints'
                    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
                    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                                      discriminator_optimizer=discriminator_optimizer,
                                                      generator=generator,
                                                      discriminator=discriminator)
                    
                    EPOCHS = 50
                    noise_dim = 100
                    num_examples_to_generate = 16
                    
                    seed = tf.random.normal([num_examples_to_generate, noise_dim])
                    
                    train(train_dataset, EPOCHS)
                    </pre>
            <br><br>
            </blockquote>
        </p>
    </div>
    
    <footer>
        <ul>
            <li class="nav-item"><a href="../index.html#theory">Teoria</a></li>
            <li class="nav-item"><a href="../index.html#research">Badania</a></li>
            <li class="nav-item"><a href="../index.html#uath">Autorki</a></li>
            <li class="nav-item"><a href="#" onclick="return slowScroll('#code')">Kod źródłowy</a></li>
        </ul>
    </footer>  
      <script src="https://kit.fontawesome.com/9f85493fa2.js" crossorigin="anonymous"></script>
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
      <script src="../js/main.js"></script>
      <script src="../js/menu.js"></script>
</body>
</html>